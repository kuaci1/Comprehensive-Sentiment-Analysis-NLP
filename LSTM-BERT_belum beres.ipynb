{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1864df-6831-4ed1-8917-608555e5cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] Using device: cpu\n",
      "[Data] Train size: 585 | Test size: 147\n",
      "\n",
      "==============================\n",
      "BUILDING MODEL A: LSTM\n",
      "==============================\n",
      "[LSTM] Vocabulary size: 908 unique words\n",
      "\n",
      "==============================\n",
      "BUILDING MODEL B: BERT\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e169ef837a2a4c0592a67b4cf42e5b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea61cad2d64eb88c55eb83d80db085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d4fb68f36a46da9e48380a9345f219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8b46776850494eb6faf09d7e38bc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12d43acabc143bdb7c705cb2c2946b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training] Starting LSTM for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.6647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Avg Loss: 0.6651\n",
      "\n",
      "[LSTM] Validation Accuracy: 75.51%\n",
      "\n",
      "[Training] Starting BERT for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.3608\n",
      "\n",
      "[BERT] Validation Accuracy: 79.59%\n",
      "\n",
      "==============================\n",
      "FINAL RESULT COMPARISON\n",
      "==============================\n",
      "1. LSTM Accuracy : 75.51%\n",
      "2. BERT Accuracy : 79.59%\n",
      "\n",
      "Conclusion: BERT outperformed LSTM by 4.08%.\n",
      "Reason: BERT uses 'Attention Mechanism' to understand context better than LSTM's sequential processing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI UMUM ---\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "LSTM_EPOCHS = 5\n",
    "BERT_EPOCHS = 3\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"[System] Using device: {DEVICE}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA PREPARATION UTILS\n",
    "# ==========================================\n",
    "class DataHandler:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "    \n",
    "    def load_and_clean(self):\n",
    "        # Load Data\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.filepath)\n",
    "        except:\n",
    "            self.df = pd.read_csv('archive (8).zip/sentimentdataset.csv')\n",
    "            \n",
    "        # Clean Whitespace\n",
    "        for col in self.df.select_dtypes(include=['object']).columns:\n",
    "            self.df[col] = self.df[col].str.strip()\n",
    "            \n",
    "        # Normalize Labels\n",
    "        def map_label(txt):\n",
    "            txt = str(txt).lower()\n",
    "            if any(x in txt for x in ['positive', 'happy', 'joy', 'love', 'excitement']):\n",
    "                return 2 # Positive\n",
    "            elif any(x in txt for x in ['negative', 'sad', 'anger', 'hate', 'fear']):\n",
    "                return 0 # Negative\n",
    "            return 1 # Neutral\n",
    "            \n",
    "        self.df['label'] = self.df['Sentiment'].apply(map_label)\n",
    "        \n",
    "        # Simple text cleaning for LSTM\n",
    "        def clean_text(text):\n",
    "            text = str(text).lower()\n",
    "            text = re.sub(r'[^a-z0-9\\s]', '', text) # Hapus simbol aneh\n",
    "            return text\n",
    "            \n",
    "        self.df['clean_text'] = self.df['Text'].apply(clean_text)\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Load Data Sekali Saja\n",
    "handler = DataHandler('sentimentdataset.csv')\n",
    "df = handler.load_and_clean()\n",
    "\n",
    "# Split Data\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    df['clean_text'], df['label'], test_size=0.2, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"[Data] Train size: {len(X_train_raw)} | Test size: {len(X_test_raw)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL A: LSTM (Long Short-Term Memory)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"BUILDING MODEL A: LSTM\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# A. Build Vocabulary (Manual Tokenization)\n",
    "class Vocabulary:\n",
    "    def __init__(self, texts, min_freq=2):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.min_freq = min_freq\n",
    "        self.build_vocab(texts)\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(text.split())\n",
    "            \n",
    "        idx = 2\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "    def encode(self, text, max_len):\n",
    "        tokens = text.split()\n",
    "        indices = [self.word2idx.get(w, 1) for w in tokens] # 1 is UNK\n",
    "        \n",
    "        # Padding / Truncating\n",
    "        if len(indices) < max_len:\n",
    "            indices += [0] * (max_len - len(indices)) # 0 is PAD\n",
    "        else:\n",
    "            indices = indices[:max_len]\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Init Vocab based on Train Data\n",
    "vocab = Vocabulary(X_train_raw)\n",
    "print(f\"[LSTM] Vocabulary size: {len(vocab)} unique words\")\n",
    "\n",
    "# B. LSTM Dataset\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.vocab.encode(self.texts[idx], self.max_len)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# C. LSTM Architecture\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, output_dim=3):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, max_len]\n",
    "        embedded = self.embedding(x) \n",
    "        # embedded shape: [batch_size, max_len, embed_dim]\n",
    "        \n",
    "        # LSTM Output\n",
    "        # output: [batch, len, hidden], (hidden_state, cell_state)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Ambil hidden state terakhir dari layer terakhir\n",
    "        # hidden shape: [1, batch_size, hidden_dim] -> squeeze -> [batch_size, hidden_dim]\n",
    "        last_hidden = hidden[-1]\n",
    "        \n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n",
    "\n",
    "# D. Setup LSTM Training\n",
    "train_ds_lstm = LSTMDataset(X_train_raw, y_train, vocab, MAX_LEN)\n",
    "test_ds_lstm = LSTMDataset(X_test_raw, y_test, vocab, MAX_LEN)\n",
    "\n",
    "train_loader_lstm = DataLoader(train_ds_lstm, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_lstm = DataLoader(test_ds_lstm, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_lstm = SimpleLSTM(vocab_size=len(vocab)).to(DEVICE)\n",
    "optim_lstm = Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL B: BERT (Transformer)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"BUILDING MODEL B: BERT\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# A. Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# B. BERT Dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# C. Setup BERT Training\n",
    "train_ds_bert = BERTDataset(X_train_raw, y_train, tokenizer, MAX_LEN)\n",
    "test_ds_bert = BERTDataset(X_test_raw, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader_bert = DataLoader(train_ds_bert, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_ds_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Menggunakan BERT Base (lebih powerful dari DistilBERT, tapi lebih berat)\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model_bert.to(DEVICE)\n",
    "optim_bert = AdamW(model_bert.parameters(), lr=2e-5)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING ENGINE (Unified)\n",
    "# ==========================================\n",
    "def train_engine(model_name, model, loader, optimizer, epochs, is_bert=False):\n",
    "    print(f\"\\n[Training] Starting {model_name} for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if is_bert:\n",
    "                # BERT Input\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                # LSTM Input\n",
    "                x, y = batch\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                preds = model(x)\n",
    "                loss = criterion(preds, y)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "def evaluate_engine(model_name, model, loader, is_bert=False):\n",
    "    model.eval()\n",
    "    preds_list, labels_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if is_bert:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                outputs = model(input_ids, attention_mask=mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "            else:\n",
    "                x, y = batch\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                logits = model(x)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                labels = y\n",
    "            \n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(labels_list, preds_list)\n",
    "    print(f\"\\n[{model_name}] Validation Accuracy: {acc*100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# ==========================================\n",
    "# 5. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# --- RUN LSTM ---\n",
    "train_engine(\"LSTM\", model_lstm, train_loader_lstm, optim_lstm, LSTM_EPOCHS, is_bert=False)\n",
    "acc_lstm = evaluate_engine(\"LSTM\", model_lstm, test_loader_lstm, is_bert=False)\n",
    "\n",
    "# --- RUN BERT ---\n",
    "train_engine(\"BERT\", model_bert, train_loader_bert, optim_bert, BERT_EPOCHS, is_bert=True)\n",
    "acc_bert = evaluate_engine(\"BERT\", model_bert, test_loader_bert, is_bert=True)\n",
    "\n",
    "# ==========================================\n",
    "# 6. FINAL COMPARISON\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"FINAL RESULT COMPARISON\")\n",
    "print(\"=\"*30)\n",
    "print(f\"1. LSTM Accuracy : {acc_lstm*100:.2f}%\")\n",
    "print(f\"2. BERT Accuracy : {acc_bert*100:.2f}%\")\n",
    "\n",
    "if acc_bert > acc_lstm:\n",
    "    diff = acc_bert - acc_lstm\n",
    "    print(f\"\\nConclusion: BERT outperformed LSTM by {diff*100:.2f}%.\")\n",
    "    print(\"Reason: BERT uses 'Attention Mechanism' to understand context better than LSTM's sequential processing.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: LSTM performed comparably to BERT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc074ff-ddae-40b1-bcf9-e8689133c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ DEEP LEARNING BENCHMARK RESULTS üèÜ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21532\\4178674546.py:16: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(comparison_table.style.applymap(lambda x: 'background-color: lightgreen' if '79.59%' in str(x) else '', subset=['Accuracy']))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a818d_row1_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a818d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a818d_level0_col0\" class=\"col_heading level0 col0\" >Model Architecture</th>\n",
       "      <th id=\"T_a818d_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_a818d_level0_col2\" class=\"col_heading level0 col2\" >Difference</th>\n",
       "      <th id=\"T_a818d_level0_col3\" class=\"col_heading level0 col3\" >Mechanism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a818d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a818d_row0_col0\" class=\"data row0 col0\" >Custom LSTM</td>\n",
       "      <td id=\"T_a818d_row0_col1\" class=\"data row0 col1\" >75.51%</td>\n",
       "      <td id=\"T_a818d_row0_col2\" class=\"data row0 col2\" >-</td>\n",
       "      <td id=\"T_a818d_row0_col3\" class=\"data row0 col3\" >Sequential (RNN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a818d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a818d_row1_col0\" class=\"data row1 col0\" >DistilBERT</td>\n",
       "      <td id=\"T_a818d_row1_col1\" class=\"data row1 col1\" >79.59%</td>\n",
       "      <td id=\"T_a818d_row1_col2\" class=\"data row1 col2\" >+ 4.08%</td>\n",
       "      <td id=\"T_a818d_row1_col3\" class=\"data row1 col3\" >Self-Attention (Transformer)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2013edc4050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data hasil training kamu\n",
    "results = {\n",
    "    'Model Architecture': ['Custom LSTM', 'DistilBERT'],\n",
    "    'Accuracy': ['75.51%', '79.59%'],\n",
    "    'Difference': ['-', '+ 4.08%'],\n",
    "    'Mechanism': ['Sequential (RNN)', 'Self-Attention (Transformer)']\n",
    "}\n",
    "\n",
    "# Buat DataFrame\n",
    "comparison_table = pd.DataFrame(results)\n",
    "\n",
    "# Tampilkan dengan style highlight\n",
    "print(\"üèÜ DEEP LEARNING BENCHMARK RESULTS üèÜ\")\n",
    "display(comparison_table.style.applymap(lambda x: 'background-color: lightgreen' if '79.59%' in str(x) else '', subset=['Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478893fd-75d5-476a-b218-e2eac4a1ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] Initiating save process to './saved_models'...\n",
      "[BERT] Saving model artifacts to ./saved_models\\distilbert_v1...\n",
      "[BERT] Successfully saved.\n",
      "[LSTM] Saving model artifacts to ./saved_models\\custom_lstm_v1...\n",
      "[LSTM] Successfully saved.\n",
      "[System] All processes completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Konfigurasi Path Penyimpanan\n",
    "BASE_DIR = './saved_models'\n",
    "BERT_PATH = os.path.join(BASE_DIR, 'distilbert_v1')\n",
    "LSTM_PATH = os.path.join(BASE_DIR, 'custom_lstm_v1')\n",
    "\n",
    "# Membuat folder jika belum ada\n",
    "os.makedirs(BERT_PATH, exist_ok=True)\n",
    "os.makedirs(LSTM_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"[System] Initiating save process to '{BASE_DIR}'...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. MENYIMPAN BERT (Model + Tokenizer)\n",
    "# ---------------------------------------------------------\n",
    "if 'model_bert' in globals() and 'tokenizer' in globals():\n",
    "    print(f\"[BERT] Saving model artifacts to {BERT_PATH}...\")\n",
    "    \n",
    "    # Menyimpan konfigurasi dan bobot model\n",
    "    model_bert.save_pretrained(BERT_PATH)\n",
    "    \n",
    "    # Menyimpan tokenizer (vocab.txt, config, dll)\n",
    "    tokenizer.save_pretrained(BERT_PATH)\n",
    "    \n",
    "    print(\"[BERT] Successfully saved.\")\n",
    "else:\n",
    "    print(\"[BERT] Model object not found in memory. Skipping.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. MENYIMPAN LSTM (Weights + Vocabulary + Config)\n",
    "# ---------------------------------------------------------\n",
    "if 'model_lstm' in globals() and 'vocab' in globals():\n",
    "    print(f\"[LSTM] Saving model artifacts to {LSTM_PATH}...\")\n",
    "    \n",
    "    # a. Simpan Bobot Model (State Dict)\n",
    "    weights_file = os.path.join(LSTM_PATH, 'weights.pth')\n",
    "    torch.save(model_lstm.state_dict(), weights_file)\n",
    "    \n",
    "    # b. Simpan Vocabulary (Penting untuk mapping kata -> angka)\n",
    "    vocab_file = os.path.join(LSTM_PATH, 'vocab.pkl')\n",
    "    with open(vocab_file, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "        \n",
    "    # c. Simpan Hyperparameters (Agar saat load tahu dimensinya)\n",
    "    config = {\n",
    "        'vocab_size': len(vocab),\n",
    "        'embed_dim': 100,   # Sesuaikan dengan config training\n",
    "        'hidden_dim': 128,  # Sesuaikan dengan config training\n",
    "        'output_dim': 3\n",
    "    }\n",
    "    config_file = os.path.join(LSTM_PATH, 'config.pkl')\n",
    "    with open(config_file, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "        \n",
    "    print(\"[LSTM] Successfully saved.\")\n",
    "else:\n",
    "    print(\"[LSTM] Model object not found in memory. Skipping.\")\n",
    "\n",
    "print(\"[System] All processes completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b6a5c-2dec-477d-bf97-8750a6975ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
